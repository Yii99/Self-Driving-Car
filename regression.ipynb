{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Yi Wang\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\package\\_directory_reader.py:17: UserWarning: Failed to initialize NumPy: No module named 'numpy' (Triggered internally at  ..\\torch\\csrc\\utils\\tensor_numpy.cpp:68.)\n",
      "  _dtype_to_storage = {data_type(0).dtype: data_type for data_type in _storages}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The process of our network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"att2.jpg\" width=\"480\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(torch.nn.Module):\n",
    "    def __init__(self, feature_dim, seg_dim, bias=True, **kwargs):\n",
    "        super(Attention, self).__init__(**kwargs)\n",
    "        self.bias = bias\n",
    "        self.feature_dim = feature_dim\n",
    "        self.seg_dim = seg_dim\n",
    "        self.flatten = nn.Flatten(start_dim=3, end_dim=-1)\n",
    "        self.softmax = nn.Softmax(dim=0)\n",
    "        \n",
    "        weight = torch.zeros(feature_dim) # define the shape of attention weights\n",
    "        nn.init.kaiming_uniform_(weight) # initialize it by using a normal distribution\n",
    "        self.weight = nn.Parameter(weight)  # [6*46, 1]\n",
    "        \n",
    "        if bias:\n",
    "            self.b = nn.Parameter(torch.zeros(seg_dim, 1)) # [100, 1]\n",
    "        \n",
    "    def forward(self, x):\n",
    "\n",
    "        eij = torch.matmul(self.flatten(x.contiguous()), self.weight) # Output_shape = [25, 64, 100, 1]\n",
    "        if self.bias:\n",
    "            eij = eij + self.b\n",
    "            \n",
    "        a = self.softmax(eij) # Output_shape = [25, 64, 100, 1]\n",
    "        a = torch.unsqueeze(a, -1) # Output_shape = [25, 64, 100, 1, 1]\n",
    "    \n",
    "        weighted_input = x * a # # Output_shape = [25, 64, 100, 6, 46]\n",
    "        return weighted_input\n",
    "\n",
    "class Model(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        # 1st conv layer\n",
    "        self.conv1 = nn.Conv3d(1, 8, kernel_size=[1, 3, 3], stride=1, padding=0) \n",
    "        self.mp = nn.MaxPool3d((1, 2, 2), stride=(1, 2, 2))\n",
    "        # 2nd conv layer \n",
    "        self.conv2 = nn.Conv3d(8, 16, kernel_size=[1, 3, 3], stride=1, padding=0) \n",
    "        # 3rd conv layer\n",
    "        self.conv3 = nn.Conv3d(16, 32, kernel_size=[1, 3, 3], stride=1, padding=0) \n",
    "        # 4th conv layer\n",
    "        self.conv4 = nn.Conv3d(32, 64, kernel_size=[1, 3, 3], stride=1, padding=0) \n",
    "        \n",
    "        self.attention_layer = Attention(feature_dim=[6*46, 1], seg_dim=100)\n",
    "     \n",
    "        self.flatten1 = nn.Flatten(start_dim=3, end_dim=-1)  \n",
    "        self.dense1 = nn.Linear(6*46, 1)\n",
    "        self.flatten2 = nn.Flatten(start_dim=1, end_dim=-1)\n",
    "        self.dense2 = nn.Linear(64*100, 1)\n",
    "   \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x) # Input_size = [25, 1, 100 128, 768], Output_size = [25, 8, 100 126, 764]\n",
    "        x = self.mp(x) # Input_size = [25, 8, 100 126, 764], Output_size = [25, 8, 100, 63, 383]\n",
    "\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x) # Input_size = [25, 8, 100, 63, 383], Output_size = [25, 16, 100, 61, 381]\n",
    "        x = self.mp(x) # Input_size = [25, 16, 100, 61, 381], Output_size = [25, 16, 100, 30, 190]\n",
    "        x = F.relu(x)\n",
    "\n",
    "        x = self.conv3(x) # Input_size = [25, 16, 100, 30, 190], Output_size = [25, 32, 100, 28, 188]\n",
    "        x = self.mp(x) # Input_size = [25, 32, 100, 28, 188], Output_size = [25, 32, 100, 14, 94]\n",
    "        x = F.relu(x)\n",
    "\n",
    "        x = self.conv4(x) # Input_size = [25, 32, 100, 14, 94], Output_size = [25, 64, 100, 12, 92]\n",
    "        x = self.mp(x) # Input_size = [25, 64, 100, 12, 92], Output_size = [25, 64, 100, 6, 46]\n",
    "        x = F.relu(x)\n",
    "\n",
    "        x = self.attention_layer(x) # Input_size = [25, 64, 100, 6, 46], Output_size = [25, 64, 100, 6, 46]\n",
    "\n",
    "        x = self.flatten1(x) # Input_size = [25, 64, 100, 6, 46], Output_size = [25, 64, 100, 276]\n",
    "\n",
    "        x = self.dense1(x) # Input_size = [25, 64, 100, 276], Output_size = [25, 64, 100, 1]\n",
    "\n",
    "        x = self.flatten2(x) # Input_size = [25, 64, 100, 1], Output_size = [25, 64*100]\n",
    "\n",
    "        x = self.dense2(x) # Input_size = [25, 64*100], Output_size = [25, 1]\n",
    "\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape:  torch.Size([25, 1])\n"
     ]
    }
   ],
   "source": [
    "X = torch.randn(25, 1, 100, 128, 768) #[segments, channels, tokens, embeddings]\n",
    "model = Model()\n",
    "out = model.forward(X)\n",
    "print('Output shape: ', out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:  -0.03974542021751404\n"
     ]
    }
   ],
   "source": [
    "print('Output: ', out[0].item())"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9a7ecea24861bf121dc9095de9c7a0c8d135829125aa48012e75d4bae2489b77"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
